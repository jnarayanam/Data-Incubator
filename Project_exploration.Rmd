---
title: 'Next Word Prediciton: Exploration'
author: "Jagan Narayanam"
date: "April 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

### **Synopsis:**

This report discusses the exploratory analysis of text data and future plans on training a predictive model to predict the next word in a sentence. The data for this project was downloaded from [Coursera Course website](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) and English files are used for this analysis. Initial exploration of the data (tiwtter, news and blogs) focused on understanding the data by calculating the number of words, number of lines and size of the text files. Later, as part of the "cleaning the data" used the ["quanteda"](https://cran.r-project.org/web/packages/quanteda/index.html) and ["tm"](https://cran.r-project.org/web/packages/tm/index.html) packages. However, this report only highlighted the use of "tm" package. For further analysis, a small random sample (1%) of total text was considered to represent the whole data. This analysis focused on the frequency of words (uniGgrams), biGrams, triGrams and tetraGrams. [R-weka](https://cran.r-project.org/web/packages/RWeka/index.html) package (NGramtokenizer) was used to get the n-gram statistics. This report also focused on measuring the number of words needed to cover the dictionary of the sampled text. In conclusion, this report provides an exploratory analysis and an insight on how to proceed to train a predictive model.


```{r, Loading Required Packages, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
# Required packages for this analysis
require(quanteda)
require(tm)
require(SnowballC)
require(doParallel)
require(wordcloud)
require(RWeka)
require(plyr)
require(dplyr)
require(knitr)
require(ggplot2)
```

### **Initial Exploration of the Data**
First the data was loaded into R-environment and subjected to the initial exploration to understand the data. As part of this, first calcualted the number of words, number of lines and the size of the file.

```{r initial Exploration, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
explore_text_file <- function(filename){
  con1 <- file(filename, "rb")
  content <- readLines(con1, skipNul = TRUE)
  close(con1)
  count <- 0
  for (line in content){
    words <- sapply(strsplit(line, " "), length)
    count <- count + words
  }
  size <- ceiling(file.size(filename)/1000000)
  number_of_lines <- length(content)
  return (c(count, number_of_lines, size))
}
twitter <- explore_text_file("./corpus/en_US.twitter.txt")
news <- explore_text_file("./corpus/en_US.news.txt")
blogs <- explore_text_file("./corpus/en_US.blogs.txt")
basic_info <- data.frame(rbind(twitter, news, blogs))
names(basic_info) <- c("no_of_words", "no_of_lines", "size(MB)")
```
```{r Table Priniting, echo=FALSE, cache=TRUE}
kable(basic_info, format = "markdown", align = "c")
```

The data contains more than 100 million words and the size is around 600 MB. So, to avoid the out of memory errors and lot of time consumption for processing, only a very small random sample (1%) of this data was considered (approximately 1 million words) to represent the whole data and to train the prediction model.
A function was created to sample the data of each file and and similar exploration was done on this small sample.


```{r sampling Text, echo=FALSE, cache=TRUE}
sampling_text <- function(filename1, filename2, n){
  con1 <- file(filename1, "rb") # read as binary data
  content <- readLines(con1, encoding = "UTF-8", warn = TRUE, skipNul = TRUE)
  num_of_lines = length(content)
  sampleSize = ceiling(num_of_lines * 0.02)
  set.seed(n)
  random_lines = sample(seq(1, num_of_lines), sampleSize, replace = FALSE)
  sample_text = content[random_lines]
  con2 = file(filename2, "w")
  writeLines(sample_text, con2)
  close(con1)
  close(con2)
}
sampling_text("./corpus/en_US.twitter.txt", "./corpus/txt/twitter.txt", 1234)
sampling_text("./corpus/en_US.news.txt", "./corpus/txt/news.txt", 2345)
sampling_text("./corpus/en_US.blogs.txt", "./corpus/txt/blogs.txt", 1236)
twitter_sample <- explore_text_file("./corpus/txt/twitter.txt")
news_sample <- explore_text_file("./corpus/txt/news.txt")
blogs_sample <- explore_text_file("./corpus/txt/blogs.txt")
basic_info_sample <- data.frame(rbind(twitter_sample, news_sample, blogs_sample))
names(basic_info_sample) <- c("no_of_words", "no_of_lines", "size(MB)")
kable(basic_info_sample, format = "markdown", align = "c")
```

### **Cleaning and Filtering the Corpus**

#### **with "tm" Package:**

It's time to cleanup the data into a tidy and workable data set. To do that, first created a corpus using ["tm"] (https://cran.r-project.org/web/packages/tm). For hands on experience with "tm" package see ([Hands on Data Science with R Text Mining](http://handsondatascience.com/TextMiningO.pdf)). Cleaning the text contains several steps including removal of symbols, removal of punctuations, strip white space, removal of numbers, transform all the text into lower case and conversion of non ASCII (American Standard Code for Information Interchange) character sequences. Finally filtered the profanity words from the data to avoid the appearance of pafanity words during prediction. A list of [bad words] (http://www.cs.cmu.edu/~biglou/resources/bad-words.txt) were downloaded from Carnegie Mellon University Luis von Ahn's Research Group.
```{r profanity words, echo=FALSE, cache=TRUE}
bad_con = file("./corpus/bad-words.txt", "rb")
bad_words = readLines(bad_con)
close(bad_con)
```

```{r tm Corpus Creation and Cleaning, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
cname <- file.path(".", "corpus", "txt") 
tmCorpus <- Corpus(DirSource(cname))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
tmCorpus <- tm_map(tmCorpus, PlainTextDocument)
tmCorpus <- tm_map(tmCorpus, toSpace, "/|@|\\|")
tmCorpus <- tm_map(tmCorpus, toSpace, "\\W*\\b\\w{1,1}\\b")
tmCorpus <- tm_map(tmCorpus, content_transformer(tolower)) 
tmCorpus <- tm_map(tmCorpus, removeNumbers) 
tmCorpus <- tm_map(tmCorpus, removePunctuation)
tmCorpus <- tm_map(tmCorpus, stripWhitespace)
tmCorpus <- tm_map(tmCorpus, removeWords, bad_words)
tmCorpus <- tm_map(tmCorpus, content_transformer(function(x) iconv(x, to="ASCII", sub="")))
tmCorpus <- tm_map(tmCorpus, stemDocument, language = "english")
```

### **TermDocumentMatrix and NGram Frequencies**
The next step is finding the uinique words (NGrams) and their frequencies. To achieve this created a function that can prepare a TermDocumentMatrix of the "tmCorpus" for each type of NGram. Then calculates the word frequencies and store them in a data frame with some other features such as percentage covered by each nGram and the rank of that particular nGram.  Finally, plotting the top 20 most frequent NGrams will provide the most frequent nGrams and their frequencies. 

```{r tdm Function, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
nGramFreqDf <- function(corpusData, n){
  nGramTdm <- TermDocumentMatrix(corpusData, control = 
                                   list(tokenize = function(x)  
                                   NGramTokenizer(x, RWeka::Weka_control(min = n, max = n)), 
                                   wordLengths = c(1, Inf)))
  nGramFreq <- sort(rowSums(as.matrix(nGramTdm)), decreasing = TRUE)
  
  nGramDf <- data.frame(word = names(nGramFreq), freq = nGramFreq, percentage_covered=
           nGramFreq/sum(nGramFreq), Rank = rank(-nGramFreq))
  
}
uniGramDf <- nGramFreqDf(tmCorpus, 1)
biGramDf <- nGramFreqDf(tmCorpus, 2)
triGramDf <- nGramFreqDf(tmCorpus, 3)
tetraGramDf <- nGramFreqDf(tmCorpus, 4)
uniGramDf$word <- factor(uniGramDf$word, levels= uniGramDf$word[order(uniGramDf$freq, decreasing = TRUE)])
biGramDf$word <- factor(biGramDf$word, levels = biGramDf$word[order(biGramDf$freq, decreasing = TRUE)])
triGramDf$word <- factor(triGramDf$word, levels = triGramDf$word[order(triGramDf$freq, decreasing = TRUE)])
tetraGramDf$word <- factor(tetraGramDf$word, levels = tetraGramDf$word[order(tetraGramDf$freq, decreasing = TRUE)])

```
  
**Number of Unique NGrams in the Corpus without Stopwords**

```{r table of Unique Words, echo=FALSE, cache=TRUE}
unique = c("UniGrams", "BiGrams", "TriGrams", "TetraGrams")
dfLength = c(length(uniGramDf$freq), length(biGramDf$freq), length(triGramDf$freq),
             length(tetraGramDf$freq))
kable(cbind(unique, dfLength), format = "markdown", align = "c", caption = "Number of Unique Words",
      col.names = c("NGrams Type", "Number of Ngrams"))
```


```{r, NGrams Ploting, echo=FALSE, cache=TRUE}
ggplot(data = uniGramDf[1:20, ], aes(word, freq)) + xlab("word") + ylab("freq") +
   geom_bar(stat="identity", fill = "#E69F00") + 
   theme(axis.text.x=element_text(angle=45, hjust=1))+
   ggtitle("Frequent UniGrams with Stop Words")

ggplot(data = biGramDf[1:20, ], aes(word, freq)) + xlab("word") + ylab("freq") +
   geom_bar(stat="identity", fill = "#009E73") + 
   theme(axis.text.x=element_text(angle=45, hjust=1))+
   ggtitle("Frequent BiGrams with Stop Words")

ggplot(data = triGramDf[1:20, ], aes(word, freq)) + xlab("Trigram") + ylab("Frequency") +
   geom_bar(stat="identity", fill = "#0072B2") + 
   theme(axis.text.x=element_text(angle=45, hjust=1))+
   ggtitle("Frequent TriGrams with Stop Words")
  
ggplot(data = tetraGramDf[1:20, ], aes(word, freq)) + xlab("word") + ylab("freq") +
   geom_bar(stat="identity", fill = "#D55E00") + 
   theme(axis.text.x=element_text(angle=45, hjust=1))+
   ggtitle("Frequent TetraGrams with Stop Words")
```

### **Removing the Stop Words and Repeating the Same Exercise:**  
If we look at the most frequent NGrams (uni, bi, tri and tetra) most of them are stop words. So, removing the stopwords from the corpus and repeating the same execise of generating NGrams and storing them in a dataframe followed by plotting and finding the unique NGrams in each dataframe. 

**Number of Unique NGrams in the Corpus without Stopwords**

```{r, NGrams Ploting Without StopWords, echo=FALSE, cache=TRUE}
tmCorpus_S <- tm_map(tmCorpus, removeWords, stopwords("english"))
uniGramDf_S <- nGramFreqDf(tmCorpus_S, 1)
biGramDf_S <- nGramFreqDf(tmCorpus_S, 2)
triGramDf_S <- nGramFreqDf(tmCorpus_S, 3)
tetraGramDf_S <- nGramFreqDf(tmCorpus_S, 4)

dfLength_S = c(length(uniGramDf_S$freq), length(biGramDf_S$freq), length(triGramDf_S$freq),
               length(tetraGramDf_S$freq))
kable(cbind(unique, dfLength_S), format = "markdown", align = "c", 
      caption = "Number of Unique Words without StopWords",
      col.names = c("NGrams Type", "Number of Ngrams"))

```

### **Word Cloud Plots:**
```{r wordcloud plots, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
par(mfrow=c(2,2))
wordcloud(uniGramDf_S$word, uniGramDf_S$freq, max.words =100, scale = c(2, 0.1), colors=brewer.pal(8, "Dark2"))
wordcloud(biGramDf_S$word, biGramDf_S$freq, max.words =75, scale = c(2, 0.1), colors=brewer.pal(8, "Dark2"))
wordcloud(triGramDf_S$word, triGramDf_S$freq, max.words =50, scale = c(2, 0.1), colors=brewer.pal(8, "Dark2"))
wordcloud(tetraGramDf_S$word, tetraGramDf_S$freq, max.words =50, scale = c(2, 0.1), colors=brewer.pal(8, "Dark2"))
```

### **Number of words needed for 50% and 90% text coverage**:  

To calculate this a generic function words_needed was developed. This function is useful to calculate any % of text coverage.

**Number of NGrams Needed for the Text Coverage with Stopwords**    
```{r Text Coverage, echo=FALSE, cache=TRUE}
words_needed <- function(percentage, dataset){
  total_words <- sum(dataset$freq)
  cumsum <- 0
  covered_percent <- 0
  i <- 1
  while (covered_percent<percentage) {
    cumsum <- cumsum + dataset$freq[i]
    covered_percent <- cumsum/total_words
    i <- i+1
  }
  return(i)
}
percent <- c(50, 75, 90, 95, 99)
uniGramsForCoverage <- sapply(percent, function(x) words_needed(x/100, uniGramDf))
biGramsForCoverage <- sapply(percent, function(x) words_needed(x/100, biGramDf))
triGramsForCoverage <- sapply(percent, function(x) words_needed(x/100, triGramDf))
tetraGramsForCoverage <- sapply(percent, function(x) words_needed(x/100, tetraGramDf))
kable(cbind(percent, uniGramsForCoverage, biGramsForCoverage, triGramsForCoverage, tetraGramsForCoverage),
      col.names=c("Percentage Covered", "UniGrams", "BiGrams", "TriGrams", "TetraGrams"),
      format = "markdown", align = "c",
      caption = "NGrams Needed for the Text Coverage ")


```

**Number of NGrams Needed for the Text Coverage without Stopwords**   

```{r with out stop words, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
percent <- c(50, 75, 90, 95, 99)
uniGramsForCoverage_S <- sapply(percent, function(x) words_needed(x/100, uniGramDf_S))
biGramsForCoverage_S <- sapply(percent, function(x) words_needed(x/100, biGramDf_S))
triGramsForCoverage_S <- sapply(percent, function(x) words_needed(x/100, triGramDf_S))
tetraGramsForCoverage_S <- sapply(percent, function(x) words_needed(x/100, tetraGramDf_S))
kable(cbind(percent, uniGramsForCoverage_S, biGramsForCoverage_S, triGramsForCoverage_S,
            tetraGramsForCoverage_S), 
            col.names=c("Percentage Covered", "UniGrams", "BiGrams", "TriGrams", "TetraGrams"), 
            format = "markdown", align = "c", 
            caption = "NGrams Needed for the Text Coverage without Stopwords")
```


### **Conclusion and Outlook:**

Finally, in conclusion this report describe the primary exploratory analysis of text data. This analysis consists of cleaning the data, generating the NGrams, calculating the unique NGrams generated from the corpus, the number of NGrams required for the 50% and 90% text coverage in the corpus and some word cloud plots along with the histograms of most frequent Ngrams were presented. Finally, all the nGrams were stored in dataframes.
Following are the steps considered to complete the "Next Word Prediction Capstone Project".  

*   Dealing with the misspelled words and foreign words
*   Considering some deep exploration into the data, especially to understand which data has to be considered for training the final prediction model i.e. News, Blogs, Twitter or a random sample of all of them (as shown in this report). This can be done by calculating the ratio of unique words and total words, and diversity for each type of text data in the corpus. 
*   Next, split the data in to training and test sets
*   Train prediction models such as Good-Turing and Katz Backoff model (or some other models) on trainign data
*   Test the prediciton models on test data and calculate the accuracy
*   Finally, upon acheiving the satsifactory accuracy, create a shiny app.
